{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "The data for the WeRateDogs Twitter project had many challenges in wrangling, assessing, and cleaning the data in order to create accurate interpretations and results. The wrangling required gathering data from three separate sources and file types: a CSV file that was downloaded manually, a TSV file that was downloaded and loaded into the Jupyter Notebook project using code, and gathering data directly from Twtitterâ€™s Tweepy API.  After it was wrangled and loaded into data frames, I was ready to assess the issues that may exist.\n",
    "\n",
    "The first step in assessing the data was to determine if there were missing values in critical columns in all 3 data frames.  Fortunately, all of the data was accounted for.  The next step was to check for duplicate data.  There were approximately 66 records in the Tweepy API data that were duplicates.  Next was to identify rows that were retweets rather than the original tweet record as well as those rows that were not actually about dogs.  I also discovered incorrect/non-uniform ratings.  The wrong ratings were a result of incorrect extraction of the score from the tweet post, particularly when dates were involved.  The non-uniform ratings were due to how WeRateDogs handles multiple dogs in the same photo.  Whatever rating would have been given for one dog is multiplied by the number of dogs in the picture, creating ratings like 77/70. Next, I examined the tidiness of the data.   Four columns needed to be combined, and ultimately, all three data frames needed to be merged into one master file/data frame since all columns were related.\n",
    "\n",
    "The final step was cleaning the data.  Copies of the original data frames were created to preserve the integrity of the data during the cleaning process.  Each issue that was identified in the assessing phase was defined with the steps that needed to be taken to resolve the issue, then code was created to carry out the solution, and finally the solution was tested to verify that it accomplished what it was supposed to. Structural issues and other problems in the data that resulted in dropped rows were cleaned first to prevent unnecessary work.  \n",
    "\n",
    "At the end, all of the issues were resolved, the three data frames were merged, and the final, clean data was saved and stored for future evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
